{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Error of OLS parameter\n",
    "* Suppose we got the beta value of 10\n",
    "* Is this value reliable?\n",
    "* What if the variance of this value gathered from random samples is 100? Sometimes it is like -80. Can we still believe that the true value of beta is not 0?\n",
    "* Standard error gives us the hint for this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat\\beta = \\beta_{pop} + \\displaystyle \\frac{\\sum_{i=1}^N(x_i-\\bar{x}u_i)}{Var(X)} \\\\\n",
    "Var(\\displaystyle \\frac{\\sum_{i=1}^N(x_i-\\bar{x})u_i}{Var(X)}) = \\displaystyle\\frac{\\sum_{i=1}^NVar((x_i-\\bar{x})u_i)}{Var(X)^2}~(\\because Cov=0)\\\\ = \\frac{\\sum_{i=1}^N(x_i-\\bar{x})^2\\sigma^2}{Var(X)^2} = \\frac{\\sigma^2}{Var(X)}\n",
    "$$\n",
    "\n",
    "Since we don't know the theoretical population $\\sigma^2$, we use the estimated one from the sample: $\\hat\\sigma^2$<br>\n",
    "\n",
    "$\\therefore Var(\\hat\\beta|X) = \\displaystyle\\frac{\\hat\\sigma^2}{Var(X)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to get the value of $\\hat\\sigma^2$\n",
    "\n",
    "In the sample,\n",
    "$\\tilde\\sigma^2 = \\displaystyle\\frac{1}{N}\\displaystyle\\sum_{i=1}^N\\hat{u}^2_i = \\frac{1}{N}(\\hat{u}^2_1 + \\hat{u}^2_2 + ... + \\hat{u}^2_N)$\n",
    "\n",
    "and the last k-errors are all zero when there are k regressors, because the sum of all $u_i$ should be zero if the estimator is BLUE. So,\n",
    "\n",
    "$\\tilde\\sigma^2 = \\displaystyle\\frac{1}{N-k}\\displaystyle\\sum_{i=1}^N\\hat{u}^2_i$\n",
    "\n",
    "* Intuition: if the sample variance is high relative to the total variance, then the standard error is low. It's because we have diverse elements in the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heteroskedasticy's role in standard error\n",
    "\n",
    "the variance of $\\hat\\beta$ is $\\displaystyle\\frac{\\sum_{i=1}^Nu_i^2}{Var(X)}$ since the variance is the function of X.\n",
    "\n",
    "### Serial Correlation's role in standard error\n",
    "\n",
    "the covariance between error are not zero, so the error term is added to the formula, likely to underestimate errors by the formula above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the sample variance\n",
    "* Sample variance of a sample is $\\sigma^2 = \\displaystyle\\frac{1}{N-1}\\sum_{i=1}^N(x_i-\\bar x)^2$\n",
    "* Why N-1 instead of N?\n",
    "\n",
    "proof:\n",
    "\n",
    "when the population size is N and the sample size is n,<br>\n",
    "$$\n",
    "\\bar x_s = \\displaystyle\\frac{1}{n}\\sum_{i=1}^nx_i, ~~\\mathbb{E}[\\bar X_s] = \\mu_s = \\mu_{p}\n",
    "$$<br>\n",
    "\n",
    "we want\n",
    "$$\n",
    "\\sigma^2_{p} = \\displaystyle\\frac{1}{n}((x_1-\\bar x_s)^2+(x_1-\\bar x_s)^2+...+(x_{n}-\\bar x_s)^2)\n",
    "$$<br>\n",
    "then<br>\n",
    "$$\n",
    "\\mathbb{E}[\\sigma^2_{p}] = \\displaystyle\\frac{1}{n}(\\sigma_s^2+\\sigma_s^2+...+0)=\\frac{n-1}{n}\\sigma^2_s (\\because \\mu_s = \\mu_p)\\\\\n",
    "\\therefore \\sigma_s^2 = \\frac{n}{n-1}\\sigma^2 = \\frac{n}{n-1}\\frac{1}{n}\\sum_{i=1}^n(x_i-\\bar x_s)^2 = \\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar x_s)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing\n",
    "\n",
    "**What is hypothesis testing?**\n",
    "* Statistics is all about confidence.\n",
    "* In most time, we can't figure out the statistic of the whole population, and should deal with samples.\n",
    "* Then, can we figure out the properties of the whole population, when we only have a small size of samples? Are the properties derived from those samples reliable? In other words, will those can be applied to the whole population too?\n",
    "* Hypothesis testing gives us answers on questions like above.\n",
    "\n",
    "\n",
    "An essential term before we start: <br>\n",
    "**Null hypothesis** : Claim that the effect being studied does not exist.\n",
    "<br>\n",
    "\n",
    "For a result to be reliable, **the null hypothesis must be rejected** with some level of (high)confidence. Hypothesis tests are all about when will we reject the null hypothesis with how much confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tail Tests\n",
    "\n",
    "Tail tests are used in hypothesis testing to determine if there is a significant difference between a sample statistic and a population parameter in a specific direction.\n",
    "\n",
    "### One-Tailed Test\n",
    "\n",
    "In a one-tailed test, we're testing for the possibility of the relationship in one direction only.\n",
    "\n",
    "1. Right-Tailed Test:\n",
    "   $$\n",
    "   H_0: \\mu \\leq \\mu_0 \\\\\n",
    "   H_1: \\mu > \\mu_0\n",
    "   $$\n",
    "\n",
    "2. Left-Tailed Test:\n",
    "   $$\n",
    "   H_0: \\mu \\geq \\mu_0 \\\\\n",
    "   H_1: \\mu < \\mu_0\n",
    "   $$\n",
    "\n",
    "### Two-Tailed Test\n",
    "\n",
    "In a two-tailed test, we're testing for the possibility of the relationship in both directions.\n",
    "\n",
    "$$\n",
    "H_0: \\mu = \\mu_0 \\\\\n",
    "H_1: \\mu \\neq \\mu_0\n",
    "$$\n",
    "\n",
    "\n",
    "## Z-Test\n",
    "\n",
    "The z-test is a statistical test used to determine whether two population means are different when the variances are known and the sample size is large.\n",
    "\n",
    "### Z-Test Statistic\n",
    "\n",
    "The z-test statistic is calculated as:\n",
    "\n",
    "$$\n",
    "z = \\frac{\\bar{x} - \\mu_0}{\\sigma / \\sqrt{n}}\n",
    "$$\n",
    "\n",
    "\n",
    "Where:\n",
    "- $\\bar{x}$ is the sample mean\n",
    "- $\\mu_0$ is the population mean under the null hypothesis\n",
    "- $\\sigma$ is the population standard deviation\n",
    "- $n$ is the sample size\n",
    "\n",
    "### Z-Test for Two Populations\n",
    "\n",
    "For comparing two population means:\n",
    "\n",
    "$$\n",
    "z = \\frac{(\\bar{x}_1 - \\bar{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\n",
    "$$\n",
    "\n",
    "\n",
    "Where:\n",
    "- $\\bar{x}_1$ and $\\bar{x}_2$ are the sample means\n",
    "- $\\mu_1$ and $\\mu_2$ are the population means\n",
    "- $\\sigma_1$ and $\\sigma_2$ are the population standard deviations\n",
    "- $n_1$ and $n_2$ are the sample sizes\n",
    "\n",
    "### Decision Rule\n",
    "\n",
    "For a significance level $\\alpha$:\n",
    "\n",
    "1. For a right-tailed test, reject $H_0$ if $z > z_{\\alpha}$\n",
    "2. For a left-tailed test, reject $H_0$ if $z < -z_{\\alpha}$\n",
    "3. For a two-tailed test, reject $H_0$ if $|z| > z_{\\alpha/2}$\n",
    "\n",
    "Where $z_{\\alpha}$ and $z_{\\alpha/2}$ are the critical values from the standard normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **T test**\n",
    "**IMPORTANT!**\n",
    "\n",
    "T-test is similar with the Z test, but used when we don't know the metrics of the whole population, and only know those from the samples. Well, In most time it's impossible to get the population metrics, so T-test is more widely used.\n",
    "\n",
    "For example, if I want to compare mean math ability between Korean and Japanese students. 10 students from each nation took the test. Korean students got mean score 71, and Japanese students got 70; can I say that Korean students are better in math, with only 1 point of difference? Maybe it is just because the sampling error. And if I say Korean students are better at math, how strong is the confidence?\n",
    "\n",
    "The Student's t-test is used to determine if there is a significant difference between the means of two groups. The test statistic is calculated as:\n",
    "\n",
    "$$\n",
    "t = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_p \\sqrt{\\frac{2}{n}}}\n",
    "$$\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\bar{x}_1$ and $\\bar{x}_2$ are the sample means\n",
    "- $s_p$ is the pooled standard deviation\n",
    "- $n$ is the sample size (assuming equal sample sizes)\n",
    "\n",
    "The pooled standard deviation is calculated as:\n",
    "\n",
    "$$\n",
    "s_p = \\sqrt{\\frac{s_1^2 + s_2^2}{2}}\n",
    "$$\n",
    "\n",
    "\n",
    "where $s_1^2$ and $s_2^2$ are the sample variances.\n",
    "\n",
    "For unequal sample sizes, the formula becomes:\n",
    "\n",
    "$$\n",
    "t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n",
    "$$\n",
    "\n",
    "\n",
    "The degrees of freedom for this test are calculated as:\n",
    "\n",
    "$$\n",
    "df = n_1 + n_2 - 2\n",
    "$$\n",
    "\n",
    "\n",
    "The null hypothesis ($H_0$) and alternative hypothesis ($H_1$) are typically stated as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "H_0&: \\mu_1 = \\mu_2 \\\\\n",
    "H_1&: \\mu_1 \\neq \\mu_2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "where $\\mu_1$ and $\\mu_2$ are the population means.\n",
    "\n",
    "The calculated t-value is compared to the critical t-value from the t-distribution table, or the p-value is computed to make a decision about the null hypothesis.\n",
    "\n",
    "So in the example of math score above, I need the variation of scores for each of the sample to confirm if there is a significant difference or not.\n",
    "\n",
    "**How can we use T-test to evaluate regression model?**\n",
    "1. Set null hypothesis $H_{0k} : \\beta_k = 0$\n",
    "2. Perform T test for that null hypothesis\n",
    "3. If the null hypothesis is rejected, then beta is not zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **F test**\n",
    "\n",
    "T test is powerful to measure how significant is the mean difference. However it can compare two distributions, not many. Maybe we should consider the joint effect of the regressors, not the individual ones, when estimating the power of the regression model.\n",
    "\n",
    "The F-test is used to compare the variances of two populations or to test the overall significance of a regression model. The test statistic follows an F-distribution under the null hypothesis.\n",
    "\n",
    "### F-Statistic\n",
    "\n",
    "The F-statistic is calculated as the ratio of two variances:\n",
    "\n",
    "$$\n",
    "F = \\frac{s_1^2}{s_2^2}\n",
    "$$\n",
    "\n",
    "\n",
    "where $s_1^2$ and $s_2^2$ are the sample variances.\n",
    "\n",
    "### Degrees of Freedom\n",
    "\n",
    "The F-distribution has two parameters for degrees of freedom:\n",
    "\n",
    "- $df_1 = n_1 - 1$ (numerator degrees of freedom)\n",
    "- $df_2 = n_2 - 1$ (denominator degrees of freedom)\n",
    "\n",
    "where $n_1$ and $n_2$ are the sample sizes.\n",
    "\n",
    "### Hypothesis Testing\n",
    "\n",
    "For a two-tailed test of equal variances:\n",
    "\n",
    "- Null hypothesis: $H_0: \\sigma_1^2 = \\sigma_2^2$\n",
    "- Alternative hypothesis: $H_1: \\sigma_1^2 \\neq \\sigma_2^2$\n",
    "\n",
    "### Critical Value\n",
    "\n",
    "The critical value $F_{\\alpha, df_1, df_2}$ is found from the F-distribution table or calculator, where $\\alpha$ is the significance level.\n",
    "\n",
    "### Decision Rule\n",
    "\n",
    "- If $F < F_{\\alpha/2, df_1, df_2}$ or $F > F_{1-\\alpha/2, df_1, df_2}$, reject $H_0$\n",
    "- Otherwise, fail to reject $H_0$\n",
    "\n",
    "### F-Test in Regression\n",
    "\n",
    "* F-test for regression is essentially testing if the regression using x to get y is better than just estimating y with the previous mean of y.\n",
    "\n",
    "We need some metrics for this:<br>\n",
    "Restricted Sum of Squares for Regression $SSR_R = \\displaystyle\\sum_{i=1}^N(y_i-\\bar{y})^2$<br>\n",
    "Unrestricted Sum of Squares for Regression $SSR_{UR} = \\displaystyle\\sum_{i=1}^N(y_i-\\hat{y})^2$<br>\n",
    "and the F statistic is:\n",
    "$$F = \\frac{(SSR_R-SSR_{UR})/P_R}{SSR_{UR}/(N-P-1)}$$\n",
    "where $P_R$ is the first degree fo freedom which is the number of linear restrictions in $H_0$ same with the number of zero betas. $P$ is the original # of regressors, which makes up $N-P-1$ which is the second degree of freedom.\n",
    "We can test for the meaninglessness of betas for any combinations of betas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Normality Tests**\n",
    "\n",
    "## 1. Jaques - Bera Test\n",
    "* Used to prove the error term from a regressor is normally distributed.\n",
    "* Originally it is just a normality test\n",
    "* **Uses skewness and kurtosis to figure out if the distribution is similar to the normal distribution.**\n",
    "\n",
    "The Jacques-Bera test statistic is:\n",
    "\n",
    "$$\n",
    "JB = \\frac{n}{6}\\left(S^2 + \\frac{1}{4}(K-3)^2\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "- $n$ is the number of observations\n",
    "- $S$ is the sample skewness\n",
    "- $K$ is the sample kurtosis\n",
    "\n",
    "The sample skewness $S$ and kurtosis $K$ are calculated as:\n",
    "\n",
    "$$\n",
    "S = \\frac{\\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^3}{(\\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^2)^{3/2}}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "K = \\frac{\\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^4}{(\\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^2)^2}\n",
    "$$\n",
    "\n",
    "\n",
    "Under the null hypothesis of normality, the JB statistic asymptotically follows a chi-squared distribution with two degrees of freedom:\n",
    "\n",
    "$$\n",
    "JB \\sim \\chi^2_2\n",
    "$$\n",
    "\n",
    "\n",
    "The null hypothesis is rejected if the JB statistic is greater than the critical value from the chi-squared distribution with 2 degrees of freedom at the chosen significance level(generally 0.95 or 0.99)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Shapiro-Wilk test\n",
    "\n",
    "* Also a test that can be used to figure out if the error tern is normally distributed.\n",
    "* Uses the quantiles - quite similar with the QQplot, but in numerical metrics.\n",
    "\n",
    "The Shapiro-Wilk test is a statistical test of the hypothesis that sample data have the skewness and kurtosis matching a normal distribution. It is defined as:\n",
    "\n",
    "$$\n",
    "W = \\frac{(\\sum_{i=1}^n a_i x_{(i)})^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "- $x_{(i)}$ are the ordered sample values (x_(1) is the smallest)\n",
    "- $\\bar{x}$ is the sample mean\n",
    "- $a_i$ are constants generated from the means, variances and covariances of the order statistics of a sample of size n from a normal distribution\n",
    "\n",
    "The coefficients $a_i$ are calculated as:\n",
    "\n",
    "$$\n",
    "(a_1, \\ldots, a_n) = \\frac{m^T V^{-1}}{(m^T V^{-1} V^{-1} m)^{1/2}}\n",
    "$$\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "- $m = (m_1, \\ldots, m_n)^T$\n",
    "- $m_i$ are the expected values of the order statistics of independent and identically distributed random variables sampled from the standard normal distribution\n",
    "- $V$ is the covariance matrix of those order statistics\n",
    "\n",
    "The null hypothesis of this test is that the population is normally distributed. Thus, if the p-value is less than the chosen alpha level, then the null hypothesis is rejected and there is evidence that the data tested are not from a normally distributed population.\n",
    "\n",
    "The W statistic is always greater than zero and less than or equal to one, with a value of one indicating normality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Kolmogorov-Smirnov Test\n",
    "\n",
    "* Uses the difference between CDFs between two distributions\n",
    "* Can be used to compare with any arbitrary distributions.\n",
    "\n",
    "The Kolmogorov-Smirnov (K-S) test is a nonparametric test of the equality of continuous, one-dimensional probability distributions that can be used to compare a sample with a reference probability distribution or to compare two samples.\n",
    "\n",
    "### One-Sample K-S Test\n",
    "\n",
    "For a one-sample K-S test, the test statistic is defined as:\n",
    "\n",
    "$$\n",
    "D_n = \\sup_x |F_n(x) - F(x)|\n",
    "$$\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "- $F_n(x)$ is the empirical distribution function of the sample\n",
    "- $F(x)$ is the cumulative distribution function of the reference distribution\n",
    "- $\\sup_x$ is the supremum of the set of distances\n",
    "\n",
    "### Two-Sample K-S Test\n",
    "\n",
    "For a two-sample K-S test, the test statistic is:\n",
    "\n",
    "$$\n",
    "D_{n,m} = \\sup_x |F_{1,n}(x) - F_{2,m}(x)|\n",
    "$$\n",
    "\n",
    "\n",
    "where $F_{1,n}$ and $F_{2,m}$ are the empirical distribution functions of the first and second sample respectively, and $n$ and $m$ are the sample sizes.\n",
    "\n",
    "### Critical Values\n",
    "\n",
    "The critical value for the K-S test statistic is given by:\n",
    "\n",
    "$$\n",
    "D_{\\alpha} = c(\\alpha) \\sqrt{\\frac{n+m}{nm}}\n",
    "$$\n",
    "\n",
    "\n",
    "where $c(\\alpha)$ is the coefficient that depends on the significance level $\\alpha$. For $\\alpha = 0.05$, $c(\\alpha) \\approx 1.36$.\n",
    "\n",
    "### Null Hypothesis\n",
    "\n",
    "The null hypothesis of the K-S test is:\n",
    "\n",
    "$$\n",
    "H_0: F_1(x) = F_2(x) \\text{ for all } x\n",
    "$$\n",
    "\n",
    "\n",
    "The null hypothesis is rejected if the test statistic $D$ is greater than the critical value $D_{\\alpha}$.\n",
    "\n",
    "### Asymptotic Distribution\n",
    "\n",
    "For large sample sizes, the asymptotic distribution of the K-S test statistic under the null hypothesis is given by:\n",
    "\n",
    "$$\n",
    "\\lim_{n \\to \\infty} P(\\sqrt{n}D_n \\leq x) = 1 - 2\\sum_{k=1}^{\\infty} (-1)^{k-1}e^{-2k^2x^2}\n",
    "$$\n",
    "\n",
    "\n",
    "This distribution is known as the Kolmogorov distribution."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
