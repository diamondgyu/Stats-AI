{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1jKDqFUi0F8jFpEEeaNc_r-6jjIeUwIrx","timestamp":1716452009733}],"authorship_tag":"ABX9TyOuJjfXnf/5rBpFQv2oTfUA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Terms**\n","\n","**Population** : group on which information is being gathered and analyzed\n","\n","**Sample** : Small, mamageable version of population  \n","\n","**Unbiasedness** : $\\mathbb{E}[\\hat{\\beta}] = \\beta_{pop}$\n","\n","**Consistency** : as $n\\rightarrowâˆž, \\hat{\\beta}=\\beta_{pop}$  \n","\n","**Efficiency** : How many samples are needed to get $\\hat\\beta$ cetered\n","* Don't have to be centered to true value.  \n","\n","<br/><br/>\n","\n","# **Ordinary Least Squares (OLS)**\n","**BLUE** : Best Linear Unbiased Estimator (Best means efficient)\n","\n","<br/><br/>\n","\n","## Proof of OLS formular\n","suppose\n","$\\hat y = \\hat\\alpha + \\hat\\beta x_i$  \n","we should minimize\n","$\\displaystyle S = \\sum_{n=1}^{N}(y_i - \\hat y_i)^2 $\n",", so\n","$\\hat\\alpha$ and $\\hat\\beta$\n","should satisfy\n","$\\displaystyle\\frac{\\partial S}{\\partial \\hat\\alpha} = 0$\n","and\n","$\\displaystyle\\frac{\\partial S}{\\partial \\hat\\beta} = 0$\n","\n","\\begin{gather}\n","\\displaystyle\\frac{\\partial S}{\\partial \\hat\\alpha} = -2\\sum_{i=0}^{N}(y_i-\\hat\\alpha-\\hat\\beta x_i)=0 \\quad \\textrm{(I)}  \\\\\n","\\displaystyle\\frac{\\partial S}{\\partial \\hat\\beta} = -2 \\sum_{i=1}^Nx_i(y_i-\\hat\\alpha-\\hat\\beta x_i)=0\\quad\\textrm{(II)}\n","\\end{gather}  \n","<br/><br/>\n","\n","* Lemma : $\\displaystyle \\sum_{i=1}^N(x_i-\\bar x)(y_i-\\bar y) = \\sum_{i=1}^Ny_i(x_i-\\bar x) = \\sum_{i=1}^N x_i(y_i-\\bar y)$  \n","Proof :\n","\\begin{aligned}\n","\\displaystyle \\sum_{i=1}^N(x_i-\\bar x)(y_i-\\bar y)& = \\sum_{i=1}^Nx_iy_i-\\bar y\\sum_{i=1}^Nx_i - \\bar x\\sum_{i=1}^Ny_i+N\\bar x \\bar y \\\\\n","&= \\sum_{i=1}^Nx_iy_i-N\\bar x \\bar y-N\\bar x \\bar y+N\\bar x \\bar y = \\sum_{i=1}^Nx_iy_i+\\bar x \\sum_{i=1}^N y_i \\\\\n","&= \\sum_{i=1}^Ny_i(x_i-\\bar x)\n","\\end{aligned}\n","and same for $\\sum_{i=1}^N x_i(y_i-\\bar y)$.  \n","\n","<br/><br/>\n","\n","\\begin{gather}\n","\\textrm{(I)} \\rightarrow \\displaystyle \\sum_{i=1}^Ny_i = \\hat\\alpha\\sum_{i=1}^N1+\\hat\\beta\\sum_{i=1}^Nx_i, \\quad so \\quad\\bar y = \\hat\\alpha + \\hat\\beta\\bar x,\\quad \\hat\\alpha=\\bar y -\\hat\\beta\\bar x\\quad \\textrm{(III)} \\\\  \n","\\textrm{(II)} \\rightarrow \\displaystyle \\sum_{i=1}^Nx_iy_i=\\hat\\alpha\\sum_{i=1}^Nx_i+\\hat\\beta\\sum_{i=1}^Nx_i^2 = (\\bar y -\\hat\\beta\\bar x)N\\bar x +\\hat\\beta\\sum_{i=1}^Nx_i^2 = N\\bar x \\bar y - \\hat\\beta N\\bar x^2 + \\hat\\beta\\sum_{i=1}^Nx_i^2 \\newline\\newline\n","\\displaystyle \\hat\\beta =\\frac{\\sum\\limits_{i=1}^Nx_iy_i-N\\bar x \\bar y}{\\sum\\limits_{i=1}^Nx_i^2-N\\bar x} = \\frac{\\sum\\limits_{i=1}^N(x_iy_i-\\bar x y_i)}{\\sum\\limits_{i=1}^N(x_i^2-\\bar x x_i)} = \\frac{\\sum\\limits_{i=1}^N(x_i-\\bar x)(y_i-\\bar y)}{\\sum\\limits_{i=1}^N(x_i-\\bar x)^2}(\\because lemma)\n","\\end{gather}\n","<br/><br/>\n","\\begin{aligned}\n","\\therefore \\quad \\hat\\beta &= \\frac{Cov(X, Y)}{Var(X)} \\\\\n","\\hat\\alpha &= \\bar y -\\hat\\beta\\bar x\n","\\end{aligned}\n","\n","<br/><br/>\n","\n","# **Moments of Random Variables**\n","$\\mathbb{E}[X^2] = \\displaystyle\\int_{-\\infty}^{+\\infty}X^2f(X)dX = \\lim_{N\\rightarrow\\infty}\\frac{X_1^2+\\cdots+X_N^2}{N}$  \n","\n","is the second moment of the distribution of random variable X.\n","\n","and $\\mathbb{E}[X^3], \\mathbb{E}[X^4], \\cdots$ are 3rd, 4th, ... moments each.\n","\n","<br></br>\n","\n","## Third Moment (Skewness)\n","TBA\n","<br></br>\n","\n","## Fourth Moment (Kurtosis)\n","TBA\n","<br></br>\n","\n","# **Covariance & Correlation**\n","\n","## Covariance\n","* Covariance shows how strong two variables change together:\n","<br></br>\n","\\begin{gather}\n","Cov(X, Y) = \\mathbb{E}[(X-\\mu_X)(Y-\\mu_Y)]\n","\\end{gather}\n","\n","$$\\begin{equation}\n","\\left\\{\n","  \\begin{aligned}\n","    x>\\mu_x : \\quad (+) \\cdotp (+) = (+)\\\\\n","    x<\\mu_x : \\quad (-) \\cdotp (-) = (+)\\\\\n","  \\end{aligned}\n","  \\right. \\quad \\textrm{when increase of x increases y}\n","\\end{equation}\n","$$\n","\n","$$\\begin{equation}\n","\\left\\{\n","  \\begin{aligned}\n","    x>\\mu_x : \\quad (+) \\cdotp (-) = (-)\\\\\n","    x<\\mu_x : \\quad (-) \\cdotp (+) = (-)\\\\\n","  \\end{aligned}\n","  \\right. \\quad \\textrm{when increase of x decreases y}\n","\\end{equation}\n","$$\n","<br></br>\n","\n","## Correlation\n","* Absolute value of covariance changes by how much large X and Y are.\n","We need to measure 'How much covariates' they are, regardless of size:\n","\\begin{gather}\n","Corr(X, Y) = \\frac{Cov(X, Y)}{\\sqrt{Var(X)Var(Y)}}\n","\\end{gather}\n","* This value changes from -1 to 1.\n","* -1 means complete negative correlation, 1 means complete positive correlation, 0 means no correlation\n","\n","# **R squared**\n","## Sum of squares\n","**Total Sum of Squares(TSS)**: $\\quad \\displaystyle\\sum_{i=1}^N(y_i-\\bar y)^2$  \n","**Explained Sum of Squares(ESS)** : $\\quad \\displaystyle\\sum_{i=1}^N(\\bar y - \\hat y_i)^2 \\rightarrow$  how much is explained  \n","**Residual Sum of Squares(RSS)** : $\\quad \\displaystyle\\sum_{i=1}^N(y_i-\\hat y_i)^2 \\rightarrow$ how much is remaining\n","<br></br>\n","## R squared\n","ESS is a good measure to check how model is explaining the data, but is can differ between numbers of data.  \n","so we introduce $R^2=\\frac{ESS}{TSS}$ to see how much of TSS is explained.  \n","However, this value can be 1 if we add arbitrary large number of regressors.  \n","For example, if $y = \\displaystyle\\sum_{i=1}^N\\beta_ix^i+\\alpha$, and $N\\rightarrow\\infty$, it can perfectly fit every points of data due to fundamental theorem of algebra. This is called (extreme) overfitting. So we need another approach to fix this.\n","<br></br>\n","## Adjusted R squared\n","Adjusted R square penalizes R square acoording to the number of regressors:\n","<br></br>\n","\\begin{gather}\n","\\textrm{Adjusted } R^2(\\bar R^2) = 1-\\frac{(1-R^2)(N-1)}{N-k-1}\n","\\end{gather}\n","when N is the sample size and k is the number of regressors.\n","<br></br>\n","# **Degrees of Freedom**\n"],"metadata":{"id":"aPUJxdII8yIf"}}]}